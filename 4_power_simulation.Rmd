---
title: 'A randomised trial to estimate the effect of funding on research productivity: Simulated power analysis'
author: "Adrian Barnett"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  word_document:
    toc: true
    toc_depth: 1
    reference_docx: rmarkdown-styles-reference.docx
---

```{r setup, include=FALSE}
# using formatting in Word document (see above)
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE, error=FALSE, comment='', dpi=400)
options(width=1000) # Wide pages
options(scipen=999) # avoid scientific presentation
source('99_functions.R')
library(dplyr)
library(tidyr)
library(ggplot2)
g.theme = theme_bw()+  theme(panel.grid.minor = element_blank())
library(janitor) # for tables with column totals
library(stringr)
library(flextable)
#library(lme4) # for glmer

# get the multiple data sets on funding and papers
source('4_get_data.R')

# number of simulations
n_sim = 100
```

In this document we use simulation to examine the statistical power of the experiment. We take the current real data and randomly allocate people to the "treatment" of funding. We then run the statistical models and examine the observed difference between groups and the width of the 95% confidence intervals. This gives us a good idea of the likely accuracy of the analysis, but without peaking at the real data. We can decide whether to run the analysis with the real treatment groups depending on the average uncertainty. Alternatively, we can wait until more follow-up time has accrued so that the statistical power and accuracy are greater.

The follow-up dates were `r format(scholar_date, '%d-%b-%Y')` for _Google Scholar_, `r format(scopus_date, '%d-%b-%Y')` for _Scopus_ and `r format(researchgate_date, '%d-%b-%Y')` for _researchgate_. 

We used `r n_sim` simulations.

# Statistical model

We modelled the yearly log-transformed counts of papers ($P$) using a multivariate Normal distribution for the three databases (Google scholar, Scopus and researchgate).

$$\log_e(P_{i,1:3} / d_{y(i)}) \sim \textrm{MVN}(\mu_{i,1:3}, \Sigma^2_{1:3,1:3})$$

We used the log-transform for the paper counts (dependent variable) as there was a positive skew to the count data and hence a log-transform was likely to give a better fit. 

To adjust for the most recent year of publication counts being partially complete we used:
$$ d_{y(i)} =  \{ \begin{matrix} f, & y(i) = 2022, \\ 1, & \textrm{othersise}. \end{matrix}  $$
where $0 \leq f \leq 1$, because the final year of data was collected mid-way through 2022. The value of $f$ varies by database.

The mean was modelled as:

$$\mu_{i,j} = \alpha + \delta_j + \beta \log_2(B_{i,j}) + c_{y(i)} + \gamma t_i, \qquad i=1,\ldots,N,\, j=1,2,3,$$

where $i$ is the index for the person-year and $j$ is the index for the database.
The overall intercept is $\alpha$ and $\delta_j$ is a database specific intercept to allow for more papers being included in some databases than others. The $\delta$'s are constrained so that $\delta_1+\delta_2+\delta_3 = 0$ which means they are the change from the overall intercept ($\delta$).


To adjust for differences in experience, we collected the total number of published papers for each researcher prior to randomisation ($B$). This was added using a log-transformation, and we used base 2 so that the estimates are interpretable as a doubling in baseline counts. We calculated the baseline counts per database and tested a database specific slope for the adjustment, so $\beta_j$ instead of $\beta$, but there was no great improvement in model fit, so using the parsimony principle we used the same slope for each database. 

Exploratory analysis showed a strong effect of calendar year that was non-linear. This effect could be due to the increase over time in the number of papers researchers are expected to publish. To adjust for this potential confounder of the treatment we used a smoothed effect for calendar time using a conditional autoregressive model with correlated estimates for neighbouring years. We used this non-parametric smooth because parametric smooths using fractional polynomials were not able to completely capture the non-linear changes over time.

The effect of being awarded funding was modelled using $\gamma$ with $t_i$ as the treatment for person-year $i$. In a sensitivity analysis we examined a time-varying effect for $\gamma$ as detailed below.

We used vague Normal prior distributions for the model parameters:

$$\alpha, \, \delta, \, \beta, \, \gamma \sim \textrm{N}(0, 10^4)$$

We used a multivariate Normal and assumed the log-counts from the three databases were correlated using the variance--covariance matrix:

$$\Sigma^{-2} \sim \textrm{Wishart}(3)$$

Many researchers were missing data for 1 or 2 databases. The multivariate normal distribution allowed for missing data in the counts. For missing data in the baseline counts, we imputing these counts using a second multivariate Normal data that exploited the strong correlation between databases, as researchers with high counts in one database tended to have high counts in the others.


# Model without funding

Here we fit a model without funding. The aim is to check the fit of the model for the other predictors, which are the effects of baseline paper counts and calendar time.

```{r, include=FALSE}
outfile = 'results/model_no_funding.RData'
exists = length(dir('results', pattern='model_no_funding.RData')) > 0
if(exists == FALSE){
  source('4_model_bibliographic_bayes.R')
  pilot = FALSE # run big model
  analysis_ready = add_treatment_censored(in_random = researchers, 
                                          monthly = FALSE, 
                                          in_biblio = paper_counts)
  results_no_funding = run_bayes(indata = analysis_ready, pilot = FALSE, make_residuals = TRUE) # from 4_model_bibliographic_bayes.R
  save(results_no_funding, file = outfile)
}
if(exists == TRUE){
  load(outfile)
}
```

### Table of estimates

```{r}
to_table = filter(results_no_funding$table, 
                  str_detect(parameter, pattern='beta|alpha_c')) %>% # do not show intercept
  mutate(term = case_when(
    parameter == 'int' ~ 'Intercept',
    parameter == 'beta' ~ 'Double pre-randomisation papers',
    parameter == 'alpha_c[1]' ~ 'Intercept for Scopus', 
    parameter == 'alpha_c[2]' ~ 'Intercept for Google scholar',
    parameter == 'alpha_c[3]' ~ 'Intercept for researchgate'
  ),
  pvalue = format.pval(pvalue, eps=0.0001),
    mean = 100*(exp(mean)- 1), # percent change
           lower = 100*(exp(lower)- 1),
           upper = 100*(exp(upper)- 1),
  mean = roundz(mean, 1), # rounding
         lower = roundz(lower, 1),
         upper = roundz(upper, 1),
         ci = paste(lower , ' to ' , upper, sep='')) %>%
  dplyr::select(term, mean, ci, pvalue)
ftab = flextable(to_table)%>%
  autofit() %>%
  theme_box()
ftab
```

The table shows estimated percent changes, 95% credible intervals, and Bayesian posterior p-value.

As expected there was a strong effect of the number of papers a researcher had prior to randomisation on their post-randomisation output. For the three databases, the highest number of papers was in _Google Scholar_ and the lowest in _researchgate_.

### Correlation matrix

```{r}
ftab  = flextable(results_no_funding$cormat) %>%
  colformat_double(digits=2) %>%
  autofit() %>%
  theme_box()
ftab
```


This is the correlation matrix for the multivariate normal distribution of log-counts. Scopus and Google Scholar had the strongest correlation.

### Effect of calendar time

```{r}
to_plot = results_no_funding$cplot +
  geom_point()
print(to_plot)
```

The plot shows the estimated smoothed effect of calendar time. The percent changes are the difference from the overall average. There is a clear decline in the last year, which could be a delay due to papers being added to the databases. We adjusted for the shorter time period in 2022, so the drop is not simply due to it not being a complete year.

# Assuming no effect of funding

In the next analysis we assume there is no difference between the randomised groups. We do this by randomly allocating researchers to the treatment of funding.

A key statistic is the width of the 95% credible interval of the effect of funding, as a narrow interval would indicate that we have the power to rule out a difference of practical importance. Conversely, if the credible interval is wide then there is a possibility that the real analysis would show an indeterminate result. The spread of the mean difference from the simulations also gives an indication of the potential size of any difference and whether this includes differences of practical importance. 

```{r, include=FALSE}
## make simulated data
# check if already created
created = length(dir('results', pattern='simulated.RData'))
# create simulated data
if(created == 0){
  simulated = list()
  TeachingDemos::char2seed('hartlepool')
  for (k in 1:n_sim){
    # randomly resample randomisation under null hypothesis of no treatment
    resampled = resample_random(researchers, single=FALSE) # single turned off (FALSE) means take multiple results per researcher
    # add random to biblio data; prepares the paper data for the multivariate model
    analysis_ready = add_treatment_censored(in_random = resampled, in_biblio = paper_counts) # 99_functions.R
    simulated[[k]] = analysis_ready
  }
  # check one result per person per year
  simulated[[1]] = mutate(simulated[[1]], id = paste(number, '.', year, sep=''))
  table(table(simulated[[1]]$id)) # should all be 1
# 
#analysis_ready = add_treatment(in_random = researchers, in_biblio = paper_counts)

# quick check
  cplot = ggplot(data=simulated[[1]], aes(x=career_year, y=scopus, col=funded))+
    geom_point()
#  cplot
  # save
  save(simulated, file='results/simulated.RData')
}
if(created == 1){
 load('results/simulated.RData')
}
```


We run `r n_sim` simulated data sets through the proposed model using the null hypothesis of no effect of funding on output. As well as examining the potential power of the analysis, the results will also check for unforeseen biases in the model set-up. Our original model showed a strong bias from the effect of time, and this is why we changed the design of the model to include a baseline summary of previous papers in the model rather than modelling the researchers careers over time.

```{r, include=FALSE}
## run simulated data through model
sim_outfile = 'results/simulated_results.RData'
exists = length(dir('results', pattern='simulated_results.RData')) > 0
if(exists == FALSE){
  sim_results = NULL
  pilot = TRUE
  for (sim in 1:n_sim){
    results_with_funding = run_bayes(indata = simulated[[sim]], 
                                     model_with_treatment = TRUE,
                                     pilot = TRUE)
    store = filter(results_with_funding$table, str_detect(parameter, 'gamma'))
    sim_results = bind_rows(sim_results, store)
  }
  sim_results = mutate(sim_results, sim=1:n()) # add simulation number
  save(sim_results, file=sim_outfile)
}
if(exists == TRUE){
  load(sim_outfile)
}
```

### Effects of funding from simulation

```{r, fig.height=6, fig.width=5}
# create percent change
sim_results_plot = mutate(sim_results,
           pc = 100*(exp(mean)- 1), # percent change
           lower = 100*(exp(lower)- 1),
           upper = 100*(exp(upper)- 1))
#
#label = data.frame(sim = c(0,0), pc = c(0,0), lower=0, upper=0, adj=c(1,-0.1), label = c('Fewer papers','More papers'))
#
gplot = ggplot(data=sim_results_plot, aes(x=sim, y=pc, ymin=lower, ymax=upper))+
  geom_hline(lty=2, col='dark red', yintercept=0)+
  ylab('Effect of funding (% change)')+
  xlab('Simulation')+
  geom_point()+
  geom_errorbar(width=0)+
  coord_flip()+
  g.theme 
#  geom_text(data = label, aes(x=sim, y=pc, label=label, adj=adj))
gplot
```

The plot above shows the estimated effect of treatment for 100 simulated data sets. The results are encouraging, as the plot show some simulations that show a clear treatment effect (in either direction), but the overall distribution looks to be centred on zero, meaning the model is unbiased on average.

```{r, include=FALSE}
widths = mutate(sim_results_plot, width = upper - lower) %>%
  summarise(median = median(width)) %>%
  pull(median)
```

The widths of the 95% credible intervals are also encouraging and show a reasonable accuracy, the median width on the scale of percent change is `r round(widths*10)/10`%.

```{r}
hplot = ggplot(data=sim_results_plot, aes(x=pc))+
  geom_histogram(col='grey44', fill='skyblue')+
  xlab('Percent change')+
  g.theme
hplot
```

The histogram shows the mean treatment effect for the `r n_sim` simulations. The histogram is symmetric and centered on zero. This is good news as it suggests the model is unbiased. The width of the credible intervals also look reasonable.

### Distribution of Bayesian posterior p-values from the simulation

```{r, fig.height=4, fig.width=5}
hplot = ggplot(data=sim_results_plot, aes(x=pvalue))+
  geom_histogram(col='grey22', fill='indianred1', breaks=seq(0,1,0.05))+
  xlab('P-value')+
  g.theme
hplot
#
below = sum(sim_results$pvalue < 0.05)
```

There were `r below` out of `r n_sim` p-values below the 0.05 threshold. This indicates a reasonable statistical power to detect differences depending on how 

# Model with time-varying treatment

Here we run a model with a time-varying treatment effect. We use one simulated data set to illustrate the idea. The time-varying effect was smoothed using a conditional autoregressive approach. We expect the plot to show no treatment effect as the treatment was allocated at random.

```{r, include=FALSE}
outfile = 'results/model_no_funding_time_varying.RData'
exists = length(dir('results', pattern='model_no_funding_time_varying.RData')) > 0
if(exists == FALSE){
  source('4_model_bibliographic_bayes.R')
  pilot = FALSE # run big model
  results_time_varying = run_bayes(indata = simulated[[4]], # using one simulated data 
            pilot = FALSE, 
            model_with_treatment = TRUE, # include treatment or not
            smooth_treatment = TRUE, # use smooth CAR treatment
            make_residuals = FALSE)
  save(results_time_varying, file = outfile)
}
if(exists == TRUE){
  load(outfile)
}
```

```{r}
time_varying = filter(results_time_varying$table, str_detect(parameter, pattern='gamma')) %>%
  mutate(mean = 100*(exp(mean)- 1), # percent change
      lower = 100*(exp(lower)- 1),
      upper = 100*(exp(upper)- 1),
      years_since = as.numeric(str_remove_all(parameter, pattern='[^0-9]')) - 1) # minus 1 so that first year is zero
cplot = ggplot(data=time_varying, aes(x=years_since, y=mean, ymin=lower, ymax=upper))+
    geom_hline(lty = 2, yintercept=0)+
    geom_ribbon(alpha = 0.2)+
    geom_line(size = 1.1)+
    xlab('Year since funding')+
    ylab('Percent change')+
    theme_bw()+
    theme(panel.grid.minor = element_blank())
cplot
```

The 95% credible intervals become wider as the years since funding increases because there are fewer observations and hence more uncertainty.

# Model using monthly counts

The models above used annual counts, but the funding announcements happened during the year and we assumed the treatment effect began in the year of funding. Hence some papers from the funded year were likely pre-randomisation. To somewhat account for this we used a model of monthly counts instead of annual counts. However, we could only do this for the _Scopus_ and _researchgate_ databases which had the publication date.

```{r}
outfile = 'results/model_monthly.RData'
exists = length(dir('results', pattern='model_monthly.RData')) > 0
if(exists == FALSE){
  source('4_model_yrmon_bayes.R')
  pilot = FALSE # run big model
  analysis_ready_monthly = add_treatment_censored(in_random = researchers, 
                                                  in_biblio = paper_counts_monthly,
                                                  monthly = TRUE)
  # quick fix for duplicated months/years (checked that all months and years are the same)
  analysis_ready_monthly = dplyr::select(analysis_ready_monthly, -month.x, -year.x, -year.y) %>%
    rename('month' = 'month.y')
  #
  results_yrmon_monthly = run_bayes_yrmon(indata = analysis_ready_monthly, pilot = FALSE, make_residuals = TRUE) # from 4_model_yrmon_bayes.R
  save(results_yrmon_monthly, file = outfile)
}
if(exists == TRUE){
  load(outfile)
}
```

### Table of estimates (monthly data)

```{r}
to_table = filter(results_yrmon_monthly$table, 
                  str_detect(parameter, pattern='beta|zeta|alpha_c')) %>% # do not show intercept
  mutate(term = case_when(
    parameter == 'int' ~ 'Intercept',
    parameter == 'beta' ~ 'Double pre-randomisation papers',
    parameter == 'zeta' ~ 'January effect',
    parameter == 'alpha_c[1]' ~ 'Intercept for Scopus', 
    parameter == 'alpha_c[2]' ~ 'Intercept for researchgate'
  ),
  pvalue = format.pval(pvalue, eps=0.0001),
    mean = 100*(exp(mean)- 1), # percent change
           lower = 100*(exp(lower)- 1),
           upper = 100*(exp(upper)- 1),
  mean = roundz(mean, 1), # rounding
         lower = roundz(lower, 1),
         upper = roundz(upper, 1),
         ci = paste(lower , ' to ' , upper, sep='')) %>%
  dplyr::select(term, mean, ci, pvalue)
ftab = flextable(to_table)%>%
  autofit() %>%
  theme_box()
ftab
```

The table shows estimated percent changes, 95% credible intervals, and Bayesian posterior p-value.

As expected there was a strong effect of the number of papers a researcher had prior to randomisation on their post-randomisation output. 

We added a January effect because exploratory analysis showed a clear increase in papers on this day. This is likely because some journals do not give an exact date.

### Correlation matrix (monthly data)

```{r}
ftab  = flextable(results_yrmon_monthly$cormat) %>%
  colformat_double(digits=2) %>%
  autofit() %>%
  theme_box()
ftab
```


This is the correlation matrix for the multivariate normal distribution of log-counts.

### Effect of calendar time (monthly data)

```{r}
to_plot = results_yrmon_monthly$cplot +
  geom_point()
print(to_plot)
```
